from dotenv import load_dotenv
from QR import query_rewrite
import os
import requests
import re
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import AzureSearch

# .env ë¡œë”©
load_dotenv("E:/work/MS_project_2/code/.env")

# í™˜ê²½ë³€ìˆ˜
embedding_api_key = os.getenv('Embedding_API_KEY')
embedding_endpoint = os.getenv('Embedding_ENDPOINT')
embedding_api_version = os.getenv('embedding_api_version')
embedding_deployment = os.getenv('embedding_deployment')
ai_search_endpoint = os.getenv("add_new_index_Search_ENDPOINT")
ai_search_api_key = os.getenv('AI_Search_API_KEY')
#llm_endpoint = os.getenv('OPENAI_ENDPOINT')
#llm_api_key = os.getenv('OPENAI_API_KEY')
llm_endpoint = os.getenv('OPENAI_ENDPOINT_2')
llm_api_key = os.getenv('OPENAI_API_KEY_2')

# ì„ë² ë”© ê°ì²´
embedding = AzureOpenAIEmbeddings(
    api_key = embedding_api_key,
    azure_endpoint = embedding_endpoint,
    model = embedding_deployment,
    openai_api_version = embedding_api_version
)

# ë²¡í„° ê²€ìƒ‰
def request_ai_search(query: str, source_filter: str = None, k: int = 5) -> list:
    headers = {
        "Content-Type": "application/json",
        "api-key": ai_search_api_key
    }

    query_vector = embedding.embed_query(query)

    body = {
        "search": query,
        "vectorQueries": [
            {
                "kind": "vector",
                "vector": query_vector,
                "fields": "embedding",
                "k": k
            }
        ]
    }

    if source_filter:
        cleaned_source = source_filter.replace(".pdf", "")
        body["filter"] = f"source eq '{cleaned_source}'"

    response = requests.post(ai_search_endpoint, headers=headers, json=body)

    if response.status_code != 200:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
        print(response.text)
        return []

    return [
        {
            "content": item["content"],
            "source": item.get("source", ""),
            "score": item.get("@search.score", 0)
        }
        for item in response.json()["value"]
    ]

# GPT ì‘ë‹µ ìš”ì²­
def request_gpt(prompt: str) -> str:
    headers = {
        'Content-Type': 'application/json',
        'api-key': llm_api_key
    }

    body = {
        "messages": [
            {"role": "system", "content": "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë„ìš°ë¯¸ì•¼. ì‚¬ìš©ì ì§ˆë¬¸ì— ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•´ì¤˜."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "top_p": 0.95,
        "max_tokens": 800
    }

    response = requests.post(llm_endpoint, headers=headers, json=body)
    if response.status_code == 200:
        content = response.json()['choices'][0]['message']['content']
        return re.sub(r'\[doc(\d+)\]', r'[ì°¸ì¡° \1]', content)
    else:
        print("âŒ GPT ìš”ì²­ ì‹¤íŒ¨:", response.status_code, response.text)
        return "âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ìµœì¢… RAG ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_answer_with_rag(query: str, source_filter: str = None, top_k: int = 3) -> str:
    results = request_ai_search(query, source_filter=source_filter, k=top_k)
    if not results:
        return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    #context = "\n\n".join([f"[doc{i+1}]\n{item['content']}" for i, item in enumerate(results)])
    context = "\n\n".join([f"[{item['source']}]\n{item['content']}" for item in results])
    prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ê°„ë‹¨í•˜ê³  í•µì‹¬ì ì¸ ë‹µë³€ì„ ë§Œë“¤ì–´ì¤˜.
                ë˜í•œ ë‹µë³€ì— ì–´ë–¤ ë¬¸ì„œì—ì„œ ë‚˜ì˜¨ ì •ë³´ì¸ì§€ ê°„ë‹¨íˆ ì¶œì²˜ë¥¼ ê´„í˜¸ë¡œ ë‚¨ê²¨ì¤˜.
                ì¶œì²˜ëŠ” ë¬¸ì¥ ë§¨ ìœ„ì— ë‚¨ê¸°ê³  \n í•œ ë’¤ì— ë‹µë³€
                ë‹µë³€ì€ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.
[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì°¸ê³  ë¬¸ì„œ]
{context}

ë‹µë³€:"""
    return request_gpt(prompt)

#prompt = 'ë‚˜ ê²½ê¸°ë„ ì‚¬ëŠ” ë©‹ìŸì´ ìœ¤í™ì›... ë‚˜ì´ëŠ” 26ì‚´ì´ê³  ëŒ€í•™ì€ ì¡¸ì—…í–ˆìœ¼ë‚˜ ë¬´ì§ì´ì•¼ ê·¼ë° ë‚´ì§‘ë§ˆë ¨ì„ í•˜ê³ ì‹¶ì€ë° ì¶”ì²œí• ë§Œí•œ ê³µê³ ë¬¸ ìˆì–´?'
prompt = '''ê²½ê¸°ë„ ê±°ì£¼,ë‚˜ì´ëŠ” 26ì„¸, ëŒ€í•™ ì¡¸ì—…, ë¬´ì§
            ì œì¶œí•´ì•¼í•  ì„œë¥˜'''
new_prompt = query_rewrite(prompt)
print('ğŸ¶new_prompt',new_prompt)
chunk_result = request_ai_search(new_prompt,source_filter=None)
result = generate_answer_with_rag(new_prompt,source_filter=None)

i = 1
for chunk in chunk_result:
    print('============================')
    print(f'ğŸ¤– top {i} result : {chunk}')
    i += 1
    if i == 10:
        break
    
print('============================')
print('============================')
print(f'ğŸ¤–chunk_resultğŸ¤– = {result}')
print('hi')