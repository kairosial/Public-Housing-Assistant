from dotenv import load_dotenv
from QR import query_rewrite
import os
import requests
import re
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import AzureSearch

# .env ë¡œë”©
load_dotenv()

# í™˜ê²½ë³€ìˆ˜
embedding_api_key = os.getenv('Embedding_API_KEY')
embedding_endpoint = os.getenv('Embedding_ENDPOINT')
embedding_api_version = os.getenv('embedding_api_version')
embedding_deployment = os.getenv('embedding_deployment')
ai_search_endpoint = os.getenv("pdf_vocab_gh_fixed_new_index_Search_ENDPOINT")
ai_search_api_key = os.getenv('AI_Search_API_KEY')
#llm_endpoint = os.getenv('OPENAI_ENDPOINT')
#llm_api_key = os.getenv('OPENAI_API_KEY')
llm_endpoint = os.getenv('OPENAI_ENDPOINT_2')
llm_api_key = os.getenv('OPENAI_API_KEY_2')

# ì„ë² ë”© ê°ì²´
embedding = AzureOpenAIEmbeddings(
    api_key = embedding_api_key,
    azure_endpoint = embedding_endpoint,
    model = embedding_deployment,
    openai_api_version = embedding_api_version
)

# ë²¡í„° ê²€ìƒ‰
def personal_request_ai_search(query: str, source_filter: str = None, k: int = 10) -> list:
    headers = {
        "Content-Type": "application/json",
        "api-key": ai_search_api_key
    }

    query_vector = embedding.embed_query(query)

    body = {
        "search": query,
        "vectorQueries": [
            {
                "kind": "vector",
                "vector": query_vector,
                "fields": "embedding",
                "k": k
            }
        ]
    }

    if source_filter:
        cleaned_source = source_filter.replace(".pdf", "")
        body["filter"] = f"source eq '{cleaned_source}'"

    response = requests.post(ai_search_endpoint, headers=headers, json=body)

    if response.status_code != 200:
        print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {response.status_code}")
        print(response.text)
        return []

    return [
        {
            "content": item["content"],
            "source": item.get("source", ""),
            "score": item.get("@search.score", 0)
        }
        for item in response.json()["value"]
    ]

# GPT ì‘ë‹µ ìš”ì²­
def personal_request_gpt(prompt: str) -> str:
    headers = {
        'Content-Type': 'application/json',
        'api-key': llm_api_key
    }

    body = {
        "messages": [
            {"role": "system", "content": "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë„ìš°ë¯¸ì•¼. ì‚¬ìš©ì ì§ˆë¬¸ì— ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•´ì¤˜."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "top_p": 0.95,
        "max_tokens": 800
    }

    response = requests.post(llm_endpoint, headers=headers, json=body)
    if response.status_code == 200:
        
        content = response.json()['choices'][0]['message']['content']
        return re.sub(r'\[doc(\d+)\]', r'[ì°¸ì¡° \1]', content)
    else:
        print("âŒ GPT ìš”ì²­ ì‹¤íŒ¨:", response.status_code, response.text)
        return "âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ìµœì¢… RAG ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def personal_generate_answer_with_rag(query: str, source_filter: str = None, top_k: int = 3) -> str:
    results = personal_request_ai_search(query, source_filter=source_filter, k=top_k)
    if not results:
        return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    #context = "\n\n".join([f"[doc{i+1}]\n{item['content']}" for i, item in enumerate(results)])
    context = "\n\n".join([f"[{item['source']}]\n{item['content']}" for item in results])
    prompt = f"""ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì–´ë–¤ ê³µê³ ë¬¸ì„ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ê²Œ ì¶”ì²œí•´ì¤„ ìˆ˜ ìˆëŠ”ì§€ 1ìœ„ë¶€í„° 3ìœ„ê¹Œì§€ ìˆœìœ„ë¥¼ ë§¤ê²¨ì¤˜.
                ì´í›„ ìˆœìœ„ë¥¼ ì™œ ê·¸ë ‡ê²Œ ì„¤ì •í–ˆëŠ”ì§€ ê°ê° ì´ìœ ë„ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì¤˜.
                ë‹µë³€ì€ 500ì ì´ë‚´ë¡œ ì‘ì„±í•´ì¤˜.
[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì°¸ê³  ë¬¸ì„œ]
{context}

ë‹µë³€:"""
    return personal_request_gpt(prompt)


# ìµœì¢… ê³µê³ ë¬¸ ì„ íƒ
def final_gpt(prompt,final_result):
    headers = {
        'Content-Type': 'application/json',
        'api-key': llm_api_key
    }

    body = {
        "messages": [
            {"role": "system", "content": """userì˜ ì¡°ê±´ì— ë¶€í•©í•˜ëŠ” ê³µê³ ë¬¸ì„ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•´ì¤˜. ê³µê³ ë¬¸ ì´ë¦„ë§Œì„ ì•„ë˜ [í˜•ì‹]ì„ ì§€ì¼œì„œ ë‚˜ì—´í•´ì¤˜.
 
                [í˜•ì‹]=```[ê³µê³ ë¬¸ ì´ë¦„]
                &
                [ê³µê³ ë¬¸ ì´ë¦„]
                &
                [ê³µê³ ë¬¸ ì´ë¦„]```
 
                ğŸš« ì ˆëŒ€ ë‹¤ë¥¸ í…ìŠ¤íŠ¸, ì´ìœ , ì„¤ëª…, ê¾¸ë°ˆë§, ê³µë°±, ì¤„ë°”ê¿ˆì€ ë„£ì§€ ë§ˆ.
                ğŸ“Œ ì¶œë ¥ í˜•ì‹ì´ ì´ í¬ë§·ì„ ë²—ì–´ë‚˜ë©´ ì‚¬ìš©ìê°€ ë¶ˆí•©ê²© ì²˜ë¦¬í•  ê±°ì•¼.
             
             
             """},
 
            {"role": "user", "content": f"ì¡°ê±´ : {final_result}, ì¡°ê±´ë³„ ìˆœìœ„ : {prompt}"}
        ],
        "temperature": 0.7,
        "top_p": 0.95,
        "max_tokens": 800
    }

    response = requests.post(llm_endpoint, headers=headers, json=body)
    if response.status_code == 200:
        
        content = response.json()['choices'][0]['message']['content']
        return re.sub(r'\[doc(\d+)\]', r'[ì°¸ì¡° \1]', content)
    else:
        print("âŒ GPT ìš”ì²­ ì‹¤íŒ¨:", response.status_code, response.text)
        return "âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# field_value = f'19~39ì‚´ ê³µê³  ì¶”ì²œ'
# # answer = personal_request_ai_search(field_value, source_filter=None)
# # print(answer)
# result = personal_generate_answer_with_rag(field_value,source_filter=None)
# print(result)